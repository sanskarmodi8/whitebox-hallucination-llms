# White-Box Hallucination in Large Language Models

This repository contains code, experiments, and analysis for an undergraduate research project focused on understanding and reducing hallucination behavior in large language models (LLMs) through controllable training-time and inference-time parameters.

The project adopts a **white-box, hyperparameter-driven approach**, systematically studying how different model controls influence hallucination frequency, severity, and failure modes.

---

## Research Objectives

- Analyze and categorize hallucination behaviors in large language models
- Study the impact of inference-time decoding parameters on hallucination
- Evaluate training-time interventions using parameter-efficient fine-tuning (PEFT)
- Compare decoding-based and fine-tuning-based mitigation strategies
- Quantify trade-offs between reliability, performance, and computational cost

---

## Scope of Work

This repository covers the following components:

- Hallucination analysis and taxonomy  
- Inference-time decoding controls (e.g., temperature, top-k, top-p)  
- PEFT-based fine-tuning methods (e.g., LoRA)  
- Empirical evaluation and trade-off analysis  
- Reproducible experimental workflows  

---

## Project Status

**Work in Progress**

The project is currently in its initial development phase.  
Code, experiments, and documentation will be incrementally added as the research progresses.

---

## Notes

- This repository is intended to support empirical research and reproducibility.
- Experimental results and figures will be used to support the final research paper.
- The structure of the repository may evolve as the project matures.

---

